@Book{knuth:84,
  author = 	 {Donald E. Knuth},
  title = 	 {The {\TeX} Book},
  publisher = 	 {Addison-Wesley},
  year = 	 {1984},
  edition = 	 {15th}
}

@InCollection{boulic:91,
  author = 	 {R. Boulic and O. Renault},
  title = 	 {3D Hierarchies for Animation},
  booktitle = 	 {New Trends in Animation and Visualization},
  publisher =    {John Wiley {\&} Sons ltd.},
  year = 	 {1991},
  editor = 	 {Nadia Magnenat-Thalmann and Daniel Thalmann}
}

@InCollection{smith:99,
  author = 	 {A. Smith and B. Jones},
  title = 	 {On the Complexity of Computing},
  booktitle = 	 {Advances in Computer Science},
  pages = 	 {555--566},
  publisher =    {Publishing Press},
  year = 	 {1999},
  editor = 	 {A. B. Smith-Jones}
}

@article{hpctoolkit2010,
	abstract = {Abstract HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical space–time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications. Copyright © 2009 John Wiley \& Sons, Ltd.},
	author = {Adhianto, L. and Banerjee, S. and Fagan, M. and Krentel, M. and Marin, G. and Mellor-Crummey, J. and Tallent, N. R.},
	doi = {10.1002/cpe.1553},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.1553},
	journal = {Concurrency and Computation: Practice and Experience},
	keywords = {performance tools, call path profiling, tracing, binary analysis, execution monitoring},
	number = {6},
	pages = {685–701},
	title = {{HPCT}oolkit: tools for performance analysis of optimized parallel programs},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1553},
	volume = {22},
	year = {2010}
}

@misc{petsc-web-page,
  author = {Satish Balay and Shrirang Abhyankar and Mark~F. Adams and Steven Benson and Jed Brown
    and Peter Brune and Kris Buschelman and Emil~M. Constantinescu and Lisandro Dalcin and Alp Dener
    and Victor Eijkhout and Jacob Faibussowitsch and William~D. Gropp and V\'{a}clav Hapla and Tobin Isaac and Pierre Jolivet
    and Dmitry Karpeev and Dinesh Kaushik and Matthew~G. Knepley and Fande Kong and Scott Kruger
    and Dave~A. May and Lois Curfman McInnes and Richard Tran Mills and Lawrence Mitchell and Todd Munson
    and Jose~E. Roman and Karl Rupp and Patrick Sanan and Jason Sarich and Barry~F. Smith
    and Stefano Zampini and Hong Zhang and Hong Zhang and Junchao Zhang},
  title        = {{PETS}c {W}eb page},
  url          = {https://petsc.org/},
  howpublished = {\url{https://petsc.org/}},
  year         = {2023},
}

@techreport{petsc-user-ref,
  author = {Satish Balay and Shrirang Abhyankar and Mark~F. Adams and Steven Benson and Jed Brown
    and Peter Brune and Kris Buschelman and Emil Constantinescu and Lisandro Dalcin and Alp Dener
    and Victor Eijkhout and Jacob Faibussowitsch and William~D. Gropp and V\'{a}clav Hapla and Tobin Isaac and Pierre Jolivet
    and Dmitry Karpeev and Dinesh Kaushik and Matthew~G. Knepley and Fande Kong and Scott Kruger
    and Dave~A. May and Lois Curfman McInnes and Richard Tran Mills and Lawrence Mitchell and Todd Munson
    and Jose~E. Roman and Karl Rupp and Patrick Sanan and Jason Sarich and Barry~F. Smith
    and Stefano Zampini and Hong Zhang and Hong Zhang and Junchao Zhang},
  title       = {{PETSc/TAO} Users Manual},
  institution = {Argonne National Laboratory},
  number      = {ANL-21/39 - Revision 3.19},
  year        = {2023},
}

@inproceedings{petsc-efficient,
  author    = {Satish Balay and William~D. Gropp and Lois Curfman McInnes and Barry~F. Smith},
  title     = {Efficient Management of Parallelism in Object Oriented Numerical Software Libraries},
  booktitle = {Modern Software Tools in Scientific Computing},
  editor    = {E. Arge and A.~M. Bruaset and H.~P. Langtangen},
  publisher = {Birkh{\"{a}}user Press},
  pages     = {163--202},
  year      = {1997}
}

@article{MILLS2021,
title = {Toward performance-portable {PETS}c for {GPU}-based exascale systems},
journal = {Parallel Computing},
volume = {108},
pages = {102831},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102831},
url = {https://www.sciencedirect.com/science/article/pii/S016781912100079X},
author = {Richard Tran Mills and Mark F. Adams and Satish Balay and Jed Brown and Alp Dener and Matthew Knepley and Scott E. Kruger and Hannah Morgan and Todd Munson and Karl Rupp and Barry F. Smith and Stefano Zampini and Hong Zhang and Junchao Zhang},
}

@inproceedings{zerilli2014broadband,
  title={Broadband marine {CSEM}: New benefits for subsalt and around salt exploration},
  author={Zerilli, Andrea and Labruzzo, Tiziano and Zanzi, Marco and Buonora, Marco Polo and Crepaldi, Jo{\~a}o Lucas and Menezes, Paul TL},
  booktitle={2014 SEG Annual Meeting},
  year={2014},
  organization={OnePetro}
}

@article{zerilli2016broadband,
  title={Broadband marine controlled-source electromagnetic for subsalt and around salt exploration},
  author={Zerilli, Andrea and Buonora, Marco P and Menezes, Paulo TL and Labruzzo, Tiziano and Mar{\c{c}}al, Adriano JA and Silva Crepaldi, Jo{\~a}o L},
  journal={Interpretation},
  volume={4},
  number={4},
  pages={T521--T531},
  year={2016},
  publisher={Society of Exploration Geophysicists and American Association of Petroleum~…}
}

@manual{mpi31,
    author = "{Message Passing Interface Forum}",
    title  = "{MPI}: A Message-Passing Interface Standard Version 3.1",
    url    = "https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf",
    year   = 2015,
    month  = jun
}

@article{OpenMP,
author = {Dagum, Leonardo and Menon, Ramesh},
title = {{OpenMP: An Industry-Standard API for Shared-Memory Programming}},
year = {1998},
issue_date = {January 1998},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {5},
number = {1},
issn = {1070-9924},
url = {https://doi.org/10.1109/99.660313},
doi = {10.1109/99.660313},
abstract = {The authors present a new way to achieve scalability in parallel software with OpenMP, their portable alternative to message passing. They discuss its capabilities through specific examples and comparisons with other standard parallel programming models.},
journal = {IEEE Comput. Sci. Eng.},
month = {jan},
pages = {46–55},
numpages = {10}
}


@article{castillo-reyes_petgem_2018,
	title = {{PETGEM}: {A} parallel code for {3D} {CSEM} forward modeling using edge finite elements},
	volume = {119},
	issn = {00983004},
	shorttitle = {{PETGEM}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300418301365},
	doi = {10.1016/j.cageo.2018.07.005},
	abstract = {We present the capabilities and results of the Parallel Edge-based Tool for Geophysical Electromagnetic modeling (PETGEM), as well as the physical and numerical foundations upon which it has been developed. PETGEM is an open-source and distributed parallel Python code for fast and highly accurate modeling of 3D marine controlled-source electromagnetic (3D CSEM) problems. We employ the N´ed´elec Edge Finite Element Method (EFEM) which oﬀers a good trade-oﬀ between accuracy and number of degrees of freedom, while naturally supporting unstructured tetrahedral meshes. We have particularised this new modeling tool to the 3D CSEM problem for inﬁnitesimal point dipoles asumming arbitrarily isotropic media for lowfrequencies approximations. In order to avoid source-singularities, PETGEM solves the frequency-domain Maxwell’s equations of the secondary electric ﬁeld, and the primary electric ﬁeld is calculated analytically for homogeneous background media. We assess the PETGEM accuracy using classical tests with known analytical solutions as well as recent published data of real life geological scenarios. This assessment proves that this new modeling tool reproduces expected accurate solutions in the former tests, and its ﬂexibility on realistic 3D electromagnetic problems. Furthermore, an automatic mesh adaptation strategy for a given frequency and speciﬁc source position is presented. We also include a scalability study based on fundamental metrics for high-performance computing (HPC) architectures.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Computers \& Geosciences},
	author = {Castillo-Reyes, Octavio and De La Puente, Josep and Cela, José María},
	month = oct,
	year = {2018},
	pages = {123--136},
}


@article{werthmuller_towards_2021,
	title = {Towards an open-source landscape for 3-{D} {CSEM} modelling},
	volume = {227},
	issn = {0956-540X, 1365-246X},
	url = {https://academic.oup.com/gji/article/227/1/644/6307022},
	doi = {10.1093/gji/ggab238},
	abstract = {Large-scale modelling of 3-D controlled-source electromagnetic (CSEM) surveys used to be feasible only for large companies and research consortia. This has changed over the last few years, and today there exists a selection of different open-source codes available to everyone. Using four different codes in the Python ecosystem, we perform simulations for increasingly complex models in a shallow marine setting. We ﬁrst verify the computed ﬁelds with semianalytical solutions for a simple layered model. Then we validate the responses of a more complex block model by comparing results obtained from each code. Finally, we compare the responses of a real-world model with results from the industry. On the one hand, these validations show that the open-source codes are able to compute comparable CSEM responses for challenging, large-scale models. On the other hand, they show many general and methoddependent problems that need to be faced for obtaining accurate results. Our comparison includes ﬁnite-element and ﬁnite-volume codes using structured rectilinear and octree meshes as well as unstructured tetrahedral meshes. Accurate responses can be obtained independently of the chosen method and the chosen mesh type. The runtime and memory requirements vary greatly based on the choice of iterative or direct solvers. However, we have found that much more time was spent on designing the mesh and setting up the simulations than running the actual computation. The challenging task is, irrespective of the chosen code, to appropriately discretize the model. We provide three models, each with their corresponding discretization and responses of four codes, which can be used for validation of new and existing codes. The collaboration of four code maintainers trying to achieve the same task brought in the end all four codes a signiﬁcant step further. This includes improved meshing and interpolation capabilities, resulting in shorter runtimes for the same accuracy. We hope that these results may be useful for the CSEM community at large and that we can build over time a suite of benchmarks that will help to increase the conﬁdence in existing and new 3-D CSEM codes.},
	language = {en},
	number = {1},
	urldate = {2023-04-27},
	journal = {Geophysical Journal International},
	author = {Werthmüller, Dieter and Rochlitz, Raphael and Castillo-Reyes, Octavio and Heagy, Lindsey},
	month = jul,
	year = {2021},
	pages = {644--659},
}
